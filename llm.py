from langchain_core.output_parsers  import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain.chains import create_retrieval_chain, create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from config import answer_examples
from langchain_core.runnables import RunnableLambda

store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


def get_retriever():
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large", chunk_size=149)
    index_name = 'tax-index-markdown'
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)
    retriever=database.as_retriever(search_kwargs={'k': 4})
    return retriever


def get_history_retriever():
    llm = get_llm()
    retriever = get_retriever()

    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "which might reference context in the chat history, "
        "formulate a standalone question which can be understood "
        "without the chat history. Do NOT answer the question, "
        "just reformulate it if needed and otherwise return it as is."
    )
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
   
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    return history_aware_retriever

def get_llm(model="gpt-4o"):
    llm = ChatOpenAI(model=model)
    return llm


def get_dictionary_chain():
    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template(f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
        ê±°ì£¼ìë¼ëŠ” ë§ì€ ë°˜ë“œì‹œ ë“¤ì–´ê°€ì•¼í•©ë‹ˆë‹¤. ë‹¤ë§Œ ì´ë¯¸ ê±°ì£¼ìë¼ê³  í‘œí˜„ë˜ì–´ ìˆëŠ” ê²½ìš° ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”.
        ì‚¬ì „: {dictionary}

        ì§ˆë¬¸: {{question}}
    """)
    dictionary_chain = prompt | llm | StrOutputParser()
    return dictionary_chain

def get_rag_chain():
    llm = get_llm()
    
    example_prompt = ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{answer}"),
        ]
    )
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=answer_examples,
    )

    system_prompt = (
        "ë‹¹ì‹ ì€ ì†Œë“ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì†Œë“ì„¸ë²•ì— ê´€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”"
        "ì•„ë˜ì— ì œê³µëœ ë¬¸ì„œë¥¼ í™œìš”í•´ì„œ ë‹µë³€í•´ì£¼ì‹œê³ "
        "ë‹µë³€ì„ ì•Œ ìˆ˜ ì—†ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”"
        "ë‹¤ë§Œ ì†Œë“ì„¸ì— ê´€ë ¨ëœ ë‚´ìš©ì´ì§€ë§Œ ë¬¸ì„œì— ì œê³µí•˜ê³  ìˆì§€ ì•Šë‹¤ë©´ ê°œë… ì„¤ëª…ë§Œ í•´ì£¼ì„¸ìš”."
        "ë‹µë³€ì„ ì œê³µí•  ë•ŒëŠ” ì†Œë“ì„¸ë²• (XXì¡°)ì— ë”°ë¥´ë©´ ì´ë¼ê³  ì‹œì‘í•˜ë©´ì„œ ë‹µë³€í•´ì£¼ì‹œê³ "
        "2-3 ë¬¸ì¥ ì •ë„ì˜ ì§§ì€ ë‚´ìš©ì˜ ë‹µë³€ì„ ì›í•©ë‹ˆë‹¤."
        "\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            few_shot_prompt,
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    history_aware_retriever = get_history_retriever()
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    ).pick('answer')

    return conversational_rag_chain

def route(info):
    topic = info["topic"].lower()

    if topic == "tax":
        return default_chain()  # RAGë¡œ ë‹µë³€
    if topic == "greeting":
        return "ì•ˆë…•í•˜ì„¸ìš”! ì†Œë“ì„¸ ê´€ë ¨í•´ì„œ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”. ğŸ˜Š"
    return "ì†Œë“ì„¸ë²• ê´€ë ¨ ë²•ë¥ ì— ëŒ€í•œ ì§ˆë¬¸ì´ ì•„ë‹ˆë¼ì„œ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    

def get_only_tax_chat_chain():
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template(
        """ì•„ë˜ ì§ˆë¬¸ì„ ë³´ê³  ë¶„ë¥˜ ë¼ë²¨ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        - 'tax': ì†Œë“ì„¸ë²•/ì„¸ë²•/ì†Œë“ì„¸ ê´€ë ¨ ë²•ë¥  ì§ˆë¬¸
        - 'greeting': ì§§ì€ ì¸ì‚¬/ê°ì‚¬/í˜¸ì¶œ(ì˜ˆ: ì•ˆë…•í•˜ì„¸ìš”, í•˜ì´, ê³ ë§ˆì›Œìš”, í…ŒìŠ¤íŠ¸)
        - 'other': ê·¸ ì™¸ ëª¨ë“  ê²ƒ

        ë°˜ë“œì‹œ tax/greeting/other ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥.
        ì§ˆë¬¸: {question}"""
    )
    only_tax_chat_chain = prompt | llm | StrOutputParser()
    return only_tax_chat_chain

def default_chain():
    dictionary_chain = get_dictionary_chain()
    rag_chain = get_rag_chain()

    tax_chain = {"input":dictionary_chain} | rag_chain

    return tax_chain

def get_ai_response(user_message, session_id: str):
    first_chain = get_only_tax_chat_chain()
    first_chain = first_chain.invoke({"question": user_message})

    full_chain = {
        "topic": lambda _: first_chain, 
        "question": lambda x: x["question"],
        } | RunnableLambda(route)
    
    ai_message = full_chain.stream(
        {
            "question":user_message
        }, 
        config={
            "configurable" : {
                "session_id": session_id
            }    
        }
    )
    return ai_message



# def get_ai_message(user_message):
    
#     dictionary_chain = get_dictionary_chain()
#     rag_chain = get_rag_chain()

#     tax_chain = {"input":dictionary_chain} | rag_chain
#     ai_message = tax_chain.invoke(
#         {
#             "question":user_message
#         }, 
#         config={
#             "configurable" : {
#                 "session_id": "abc123"
#             }    
#         })
#     return ai_message
